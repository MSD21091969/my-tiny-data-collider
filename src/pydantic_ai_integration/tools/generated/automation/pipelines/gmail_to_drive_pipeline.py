"""Auto-generated tool: gmail_to_drive_pipeline

Generated from: config/tools/gmail_to_drive_pipeline.yaml
DO NOT EDIT THIS FILE MANUALLY - Changes will be overwritten

This tool is part of the pydantic_ai_integration tool engineering foundation.
It was generated by the Tool Factory and automatically registered with MANAGED_TOOLS.
"""
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, List
from ...tool_decorator import register_mds_tool
from ...dependencies import MDSContext
from ..chain_executor import ChainExecutor


class GmailtodrivepipelineParams(BaseModel):
    """Parameters for gmail_to_drive_pipeline.
    
    Search Gmail messages and upload attachments to Google Drive
    """
    gmail_query: str = Field(
...,
min_length=1,        description="Gmail search query (e.g., 'from:example@gmail.com has:attachment')"
    )
    max_messages: Optional[int] = Field(
10,
ge=1,le=100,        description="Maximum number of messages to process"
    )
    drive_folder_id: str = Field(
...,
min_length=1,        description="Google Drive folder ID to upload attachments to"
    )

    class Config:
        json_schema_extra = {
            "examples": [
                {
                    "gmail_query": 'from:vendor@example.com subject:invoice has:attachment',                    "max_messages": 20,                    "drive_folder_id": '1abc_invoices_folder'                }
            ]
        }


@register_mds_tool(
    name="gmail_to_drive_pipeline",
    display_name="Gmail to Drive Pipeline",
    description="Search Gmail messages and upload attachments to Google Drive",
    category="google_workspace",
    version="1.0.0",
    tags=['gmail', 'drive', 'composite', 'pipeline', 'google_workspace', 'multi_step', 'external_api'],
    enabled=True,
    requires_auth=True,
    required_permissions=['gmail.read', 'drive.write'],
    requires_casefile=True,
    timeout_seconds=120,
    params_model=GmailtodrivepipelineParams,
    session_policies={'requires_active_session': True, 'allow_new_session': False, 'session_event_type': 'request', 'allow_session_resume': True, 'log_request_payload': True, 'log_full_response': True},
    casefile_policies={'requires_casefile': True, 'allowed_casefile_states': ['active'], 'enforce_access_control': True, 'audit_casefile_changes': True, 'create_if_missing': False},
    audit_config={'success_event': 'gmail_to_drive_pipeline.success', 'failure_event': 'gmail_to_drive_pipeline.failure', 'log_response_fields': ['messages_found', 'files_uploaded'], 'emit_casefile_event': True, 'redact_fields': []},
)
async def gmail_to_drive_pipeline(
    ctx: MDSContext,
    gmail_query: str,    max_messages: int = 10,    drive_folder_id: str) -> Dict[str, Any]:
    """Search Gmail messages and upload attachments to Google Drive
    
    Args:
        ctx: MDSContext with user_id, session_id, etc.
        gmail_query: Gmail search query (e.g., 'from:example@gmail.com has:attachment')
        max_messages: Maximum number of messages to process
        drive_folder_id: Google Drive folder ID to upload attachments to
    
    Returns:
        Dict containing execution results
    """
    # Register event for audit trail
    event_id = ctx.register_event(
        "gmail_to_drive_pipeline",
        {
            "gmail_query": gmail_query,            "max_messages": max_messages,            "drive_folder_id": drive_folder_id        }
    )
    
    # Composite tool implementation - execute chain of tools
    executor = ChainExecutor(ctx)
    
    # Prepare initial state from input parameters
    initial_state = {
        "gmail_query": gmail_query,
        "max_messages": max_messages,
        "drive_folder_id": drive_folder_id,
    }
    
    # Define chain steps
    steps = [{"inputs": {"max_results": "{{ state.max_messages }}", "query": "{{ state.gmail_query }}"}, "on_failure": {"action": "stop"}, "on_success": {"map_outputs": {"messages": "search_results"}, "next": "process_messages"}, "tool": "gmail_search_messages"}, {"inputs": {"message_id": "{{ state.search_results[0].id }}"}, "on_failure": {"action": "continue", "next": "record_results"}, "on_success": {"map_outputs": {"message": "current_message"}, "next": "upload_to_drive"}, "tool": "gmail_get_message"}, {"inputs": {"content": "{{ state.current_message }}", "mime_type": "application/json", "name": "gmail_export_{{ state.chain_id }}.json", "parent_folder_id": "{{ state.drive_folder_id }}"}, "on_failure": {"action": "retry", "max_retries": 3}, "on_success": {"map_outputs": {"file": "uploaded_file"}}, "tool": "drive_upload_file"}]
    
    # Execute chain
    chain_result = await executor.execute_chain(
        steps=steps,
        initial_state=initial_state,
        chain_name="gmail_to_drive_pipeline"
    )
    
    # Format result
    result = {
        "tool": "gmail_to_drive_pipeline",
        "status": "success" if chain_result.get("success") else "failure",
        "chain_id": chain_result.get("chain_id"),
        "steps_executed": chain_result.get("steps_executed"),
        "data": chain_result
    }
    
    
    # Update audit trail
    if ctx.tool_events:
        last_event = ctx.tool_events[-1]
        last_event.result_summary = {
            "status": "success",
            "result_preview": str(result)[:100]
        }
        last_event.status = "success"
    
    return result