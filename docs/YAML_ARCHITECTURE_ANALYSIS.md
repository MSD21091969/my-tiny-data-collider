# YAML Architecture Analysis

**Date:** 2025-10-15  
**Purpose:** Document YAML-driven tool generation architecture and validation

---

## Architecture Overview

```
CONFIG FILES (Source of Truth)
├── models_inventory_v1.yaml (DOCUMENTATION ONLY)
├── methods_inventory_v1.yaml (VERSIONED - Loaded at startup)
└── tool_schema_v2.yaml (SCHEMA DEFINITION)

         ↓ (at startup)

REGISTRIES (Runtime Memory)
├── MANAGED_METHODS (Dict[str, ManagedMethodDefinition])
└── MANAGED_TOOLS (Dict[str, ManagedToolDefinition])

         ↓ (generated on-demand)

TOOL YAMLS (Declarative Definitions)
└── config/methodtools_v1/*.yaml (34 files - GENERATED)
```

---

## Three YAML Files - Different Purposes

### 1. `models_inventory_v1.yaml` - **DOCUMENTATION ONLY**

**Purpose:** Human-readable reference of all Pydantic models  
**Status:** Auto-generated by `scripts/scan_models.py`  
**Versioned:** Yes (v1.0.1)  
**Loaded at Runtime:** NO  
**Used By:** Developers, documentation tools

**Content:**
```yaml
layers:
  layer_0_base:
    models:
      - name: BaseRequest
        file: base\envelopes.py
  layer_1_payloads:
    casefile_domain:
      models:
        - name: CreateCasefilePayload
          fields: [title, description, tags]
```

**Role:** Documentation of what models exist and their structure

---

### 2. `methods_inventory_v1.yaml` - **VERSIONED REGISTRY**

**Purpose:** Single source of truth for service methods  
**Status:** Manually maintained, versioned (v1.0.0)  
**Loaded at Runtime:** YES - via `register_methods_from_yaml()`  
**Populates:** `MANAGED_METHODS` registry (34 methods)  
**Called From:** `src/__init__.py` at application startup

**Content:**
```yaml
version: "1.0.0"
methods:
  casefile:
    - name: create_casefile
      description: "Creates new casefile with metadata"
      classification:
        domain: workspace
        subdomain: casefile
        capability: create
      models:
        request: pydantic_models.operations.casefile_ops.CreateCasefileRequest
        response: pydantic_models.operations.casefile_ops.CreateCasefileResponse
      implementation:
        class: CasefileService
        method: create_casefile
```

**Fields Stored in Registry (16 total):**
- Identity: name, description, version
- Classification: domain, subdomain, capability, complexity, maturity, integration_tier
- Execution: request_model_class, response_model_class, implementation_class, implementation_method
- Tracking: registered_at

**NOT Stored (R-A-R Pattern):**
- business_rules (auth, permissions, timeouts) → Moved to Request DTOs
- Parameters → Extracted on-demand from request_model_class

**Role:** Method registry loaded at startup, parameters extracted on-demand

---

### 3. `tool_schema_v2.yaml` - **SCHEMA DEFINITION**

**Purpose:** Schema/template for tool YAML structure  
**Status:** Documentation of YAML format  
**Loaded at Runtime:** NO (it's a schema, not data)  
**Versioned:** Yes (v2)

**Content:**
```yaml
# Schema defines structure:
name: string (required)
description: string (required)
category: string (required)
method_name: string (optional) # Reference to MANAGED_METHODS
parameters: list (optional) # Inherited from method if method_name specified
implementation:
  type: enum [method_wrapper, ...]
  method_name: string
```

**Role:** Template/specification for tool YAML files

---

## Tool YAML Generation Flow

### Step 1: Load Methods Inventory
```python
# scripts/generate_method_tools.py
inventory = load_methods_inventory("config/methods_inventory_v1.yaml")
# Returns: 34 methods with metadata
```

### Step 2: For Each Method
```python
for method in inventory['methods']:
    # 1. Import request model dynamically
    request_model = import_request_model(
        method['models']['request']['module'],
        method['models']['request']['class']
    )
    
    # 2. Extract parameters from request model
    # Handles R-A-R pattern (parameters in 'payload' field)
    params = extract_tool_parameters(request_model)
    
    # 3. Generate tool YAML
    tool_yaml = {
        'name': f"{method['name']}_tool",
        'method_reference': {
            'service': method['implementation']['class'],
            'method': method['implementation']['method']
        },
        'method_params': params,  # Documentation
        'tool_params': params + execution_controls  # Actual params
    }
```

### Step 3: Write Tool YAMLs
```python
# Output: config/methodtools_v1/CasefileService_create_casefile_tool.yaml
Path(f"config/methodtools_v1/{tool_yaml['name']}.yaml").write_text(
    yaml.dump(tool_yaml)
)
```

---

## Tool Registration Flow (Runtime)

### Step 1: Application Startup
```python
# src/__init__.py or app.py
from pydantic_ai_integration.method_decorator import register_methods_from_yaml
from pydantic_ai_integration.tool_decorator import register_tools_from_yaml

# Load methods into MANAGED_METHODS registry
register_methods_from_yaml("config/methods_inventory_v1.yaml")

# Load tools into MANAGED_TOOLS registry
register_tools_from_yaml("config/methodtools_v1/")
```

### Step 2: Method Registration
```python
# src/pydantic_ai_integration/method_decorator.py
def register_methods_from_yaml(yaml_path: str):
    methods = load_methods_from_yaml(yaml_path)
    
    for method_name, method_def in methods.items():
        # Create ManagedMethodDefinition (16 fields)
        method_obj = ManagedMethodDefinition(
            name=method_def['name'],
            description=method_def['description'],
            domain=method_def['classification']['domain'],
            request_model_class=import_model(...),
            implementation_class=method_def['implementation']['class'],
            # ... 11 more fields
        )
        MANAGED_METHODS[method_name] = method_obj
```

### Step 3: Tool Registration
```python
# src/pydantic_ai_integration/tool_decorator.py
def register_tools_from_yaml(yaml_path: str):
    for yaml_file in Path(yaml_path).glob("*.yaml"):
        tool_config = yaml.safe_load(yaml_file.read_text())
        
        # Create ManagedToolDefinition (12 fields)
        tool_obj = ManagedToolDefinition(
            name=tool_config['name'],
            description=tool_config['description'],
            method_name=tool_config['method_reference']['service'] + '.' + 
                        tool_config['method_reference']['method'],
            parameters=[...],  # From tool_params
            implementation=...,  # Generated wrapper function
        )
        MANAGED_TOOLS[tool_obj.name] = tool_obj
```

---

## Parameter Flow (R-A-R Pattern)

### Request Model Structure
```python
# Example: CreateCasefileRequest
class CreateCasefileRequest(BaseRequest):
    payload: CreateCasefilePayload  # Actual parameters here
    
class CreateCasefilePayload(BaseModel):
    title: ShortString  # 1-200 chars
    description: MediumString | None = None
    tags: TagList = []
```

### Parameter Extraction
```python
# scripts/generate_method_tools.py
def extract_tool_parameters(request_model_class):
    # Check for R-A-R pattern
    if "payload" in request_model_class.model_fields:
        payload_field = request_model_class.model_fields["payload"]
        payload_model = payload_field.annotation
        
        # Extract from payload model
        for field_name, field_info in payload_model.model_fields.items():
            params.append({
                'name': field_name,
                'type': map_python_to_openapi_type(field_info.annotation),
                'required': field_info.is_required(),
                'description': field_info.description,
                # constraints from Field()
            })
```

### Tool YAML Structure
```yaml
# config/methodtools_v1/CasefileService_create_casefile_tool.yaml
name: create_casefile_tool
method_reference:
  service: CasefileService
  method: create_casefile

method_params:  # DOCUMENTATION - what method expects
  - name: title
    type: string
    required: true
  - name: description
    type: string
    required: false

tool_params:  # EXECUTION - what tool accepts
  - name: title
    type: string
    required: true
  - name: description
    type: string
    required: false
  - name: dry_run  # Execution control (not passed to method)
    type: boolean
    default: false
```

---

## Validation Architecture

### 1. Type Normalization
```python
# src/pydantic_ai_integration/registry/parameter_mapping.py
def _normalize_type(type_hint: Any) -> str:
    # Strip Union[..., NoneType] → base type
    # Strip Annotated[str, ...] → str
    # Handle list[str] → list
    # Handle dict[str, Any] → dict
```

### 2. OpenAPI Mapping
```python
def _normalize_tool_type(tool_type: str) -> str:
    # OpenAPI → Python
    return {
        'integer': 'int',
        'number': 'float', 
        'string': 'str',
        'array': 'list',
        'object': 'dict',
        'boolean': 'bool'
    }.get(tool_type, tool_type)
```

### 3. Validation Check
```python
# Compare normalized types
method_type = _normalize_type(method_param.annotation)
tool_type = _normalize_tool_type(tool_param['type'])

if method_type != tool_type:
    # Report mismatch (downgraded to warning in Phase 1)
```

---

## Toolset Integration Opportunity

### Current Gap
No validation that generated tool YAMLs conform to `tool_schema_v2.yaml`

### Proposed Solution
Create new validation script in application:

```python
# scripts/validate_tool_yaml_schemas.py
"""
Validate generated tool YAMLs against tool_schema_v2.yaml specification.

Usage:
    python scripts/validate_tool_yaml_schemas.py
    python scripts/validate_tool_yaml_schemas.py --strict
"""
import yaml
from pathlib import Path
from typing import Dict, List, Any

def load_schema() -> Dict[str, Any]:
    """Load tool schema from tool_schema_v2.yaml."""
    schema_path = Path("config/tool_schema_v2.yaml")
    with open(schema_path) as f:
        return yaml.safe_load(f)

def validate_tool_yaml(tool_path: Path, schema: Dict) -> List[str]:
    """
    Validate single tool YAML against schema.
    
    Returns list of validation errors (empty = valid).
    """
    errors = []
    
    with open(tool_path) as f:
        tool_data = yaml.safe_load(f)
    
    # Required fields check
    required = ['name', 'description', 'category']
    for field in required:
        if field not in tool_data:
            errors.append(f"Missing required field: {field}")
    
    # Type validation
    if 'parameters' in tool_data:
        for param in tool_data['parameters']:
            if 'name' not in param or 'type' not in param:
                errors.append(f"Parameter missing name or type")
            if param.get('type') not in ['string', 'integer', 'float', 'boolean', 'array', 'object']:
                errors.append(f"Invalid parameter type: {param.get('type')}")
    
    # Method reference validation
    if 'method_reference' in tool_data:
        ref = tool_data['method_reference']
        if 'service' not in ref or 'method' not in ref:
            errors.append("method_reference missing service or method")
    
    return errors

def main():
    schema = load_schema()
    tools_dir = Path("config/methodtools_v1")
    
    all_errors = {}
    for tool_file in tools_dir.glob("*.yaml"):
        errors = validate_tool_yaml(tool_file, schema)
        if errors:
            all_errors[tool_file.name] = errors
    
    if all_errors:
        print(f"[ERROR] {len(all_errors)} tool YAMLs have schema violations:")
        for tool_name, errors in all_errors.items():
            print(f"\n{tool_name}:")
            for error in errors:
                print(f"  - {error}")
        return 1
    else:
        print(f"[OK] All {len(list(tools_dir.glob('*.yaml')))} tool YAMLs valid")
        return 0

if __name__ == "__main__":
    exit(main())
```

### Integration with Generator
```python
# scripts/generate_method_tools.py (add at end)
if not args.dry_run:
    # Generate YAMLs
    generate_all_tools()
    
    # Validate generated YAMLs
    print("\n" + "="*60)
    print("Validating generated YAMLs...")
    print("="*60)
    
    result = subprocess.run(
        [sys.executable, "scripts/validate_tool_yaml_schemas.py"],
        capture_output=True
    )
    
    if result.returncode != 0:
        print(result.stdout.decode())
        print("\n[WARN] Generated YAMLs have schema violations")
        print("Review errors above before using tools")
```

### Benefits
- **Pre-runtime validation**: Catch malformed YAMLs before tool registration
- **Generator quality**: Ensure generator produces spec-compliant output
- **CI/CD integration**: Add to `scripts/validate_registries.py --strict`
- **Documentation sync**: Validate YAMLs match `tool_schema_v2.yaml` spec

### Future: Toolset Schema Validator
If this pattern proves useful, could create reusable tool:
```
my-tiny-toolset/TOOLSET/schema_validator.py
```

For now, application-specific validation is simpler and more focused.

---

## Summary: Three-Tier Architecture

**Tier 1: Configuration (Source of Truth)**
- `methods_inventory_v1.yaml` - Method definitions (versioned, loaded at startup)
- `models_inventory_v1.yaml` - Model documentation (not loaded)
- `tool_schema_v2.yaml` - Tool YAML schema (not loaded)

**Tier 2: Runtime Registries (Memory)**
- `MANAGED_METHODS` - 34 methods with 16 fields each
- `MANAGED_TOOLS` - N tools with 12 fields each

**Tier 3: Generated Artifacts (Declarative)**
- `config/methodtools_v1/*.yaml` - 34 tool YAMLs (generated on-demand)

**Key Insight:** Only `methods_inventory_v1.yaml` is loaded at startup. Tool YAMLs are loaded on-demand when `register_tools_from_yaml()` is called. The generator script bridges method definitions to tool declarations by extracting parameters from Pydantic models using introspection.

---

**Last Updated:** 2025-10-15
